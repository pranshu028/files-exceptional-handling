{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Q1 . Discuss the scenarios where multithreading is preferable to multiprocessing and scenarios where\n        multiprocessing is a better choice",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": " When it comes to Python, choosing between multithreading and multiprocessing hinges on the specific task at hand.\n\nMultithreading shines in I/O-bound tasks, like:\n\na) Handling multiple user requests on a web server.\n\nb) Reading and writing files.\n\nc) Making multiple network requests.\n\nThese tasks often spend a lot of time waiting for external resources, so multithreading can keep your program responsive and efficient.\n\nMultiprocessing excels in CPU-bound tasks, such as:\n\na) Performing complex calculations.\n\nb) Data processing and analysis.\n\nc) Training machine learning models.\n\nThese scenarios benefit from true parallelism, as each process gets its own Python interpreter and memory space, avoiding the Global Interpreter Lock (GIL) that can hinder multithreading performance in CPU-bound tasks.\n\nUsing multithreading for I/O-bound tasks can provide better performance, whereas multiprocessing is preferable for CPU-bound tasks that require parallel computation.",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'SyntaxError'>",
          "evalue": "unmatched ')' (<ipython-input-1-77969e189c6f>, line 5)",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    a) Handling multiple user requests on a web server.\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "# Q 2.  Describe what a process pool is and how it helps in managing multiple processes efficiently.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": " A process pool in Python is a way to manage a group of worker processes for parallel execution. Imagine you have a bunch of tasks, and you can divide them among several workers, like a team tackling a list of chores.\n\nThe multiprocessing module in Python provides a Pool class to create a pool of worker processes. This helps manage and distribute tasks efficiently without manually starting and stopping processes.\n\nHere's how it helps:\n\n1. Task Distribution: You can easily distribute tasks across multiple processes. The pool takes care of assigning tasks to available workers.\n\n2. Resource Management: By reusing processes, you save the overhead of creating and destroying processes repeatedly.\n\n3. Load Balancing: The pool ensures that all processes get a fair share of tasks, leading to efficient utilization of CPU cores.\nFor example, using a process pool is straightforward with the multiprocessing.Pool class:\nfrom multiprocessing import Pool\n\ndef square(x):\n    return x * x\n\nif _name_ == \"_main_\":\n    with Pool(4) as p:\n        results = p.map(square, [1, 2, 3, 4])\n    print(results)  # Output: [1, 4, 9, 16]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Q 3  Explain what multiprocessing is and why it is used in Python programs.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": " Multiprocessing involves running multiple processes simultaneously, each with its own memory space and Python interpreter. This technique harnesses the power of multiple CPU cores, enabling true parallelism and enhancing computational efficiency.\n\nWhy is it used in Python?\n\n1. Bypassing the GIL: Python’s Global Interpreter Lock (GIL) can be a bottleneck for multi-threaded programs, limiting their ability to fully utilize multiple CPU cores. Multiprocessing circumvents this by creating separate processes.\n\n2. Parallel Execution: Ideal for CPU-bound tasks, where heavy computation is involved, such as numerical simulations or data analysis.\n\n3. Stability: Isolates processes from each other, reducing the risk of one crashing and taking down the entire program.\n\nHere's a simple Python example using the multiprocessing module:\nimport multiprocessing\n\ndef worker(num):\n    \"\"\"Thread worker function\"\"\"\n    print(f'Worker: {num}')\n\nif _name_ == '_main_':\n    jobs = []\n    for i in range(5):\n        p = multiprocessing.Process(target=worker, args=(i,))\n        jobs.append(p)\n        p.start()\n\nIn this script, multiprocessing.Process creates a new process to run the worker function. The args parameter passes arguments to the worker function. This way, each process runs independently, and you get the full power of multi-core CPUs",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Q 4 Write a Python program using multithreading where one thread adds numbers to a list, and another\nthread removes numbers from the list. Implement a mechanism to avoid race conditions using\nthreading.Lock.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "\nimport threading\nimport time\n\n# Shared resource\nnum_list = []\nlock = threading.Lock()\n\ndef add_numbers():\n    for i in range(10):\n        time.sleep(1)\n        with lock:\n            num_list.append(i)\n            print(f\"Added {i}, List: {num_list}\")\n\ndef remove_numbers():\n    for i in range(10):\n        time.sleep(1.5)\n        with lock:\n            if num_list:\n                removed = num_list.pop(0)\n                print(f\"Removed {removed}, List: {num_list}\")\n\nif _name_ == \"_main_\":\n    thread1 = threading.Thread(target=add_numbers)\n    thread2 = threading.Thread(target=remove_numbers)\n\n    thread1.start()\n    thread2.start()\n\n    thread1.join()\n    thread2.join()\n\n    print(f\"Final List: {num_list}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Q 5  Describe the methods and tools available in Python for safely sharing data between threads and\nprocesses.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "5. When it comes to sharing data between threads and processes in Python safely, several tools and methods are at your disposal. Let's break it down:\n\nFor Threads:\n1. threading.Lock:\n\nEnsures that only one thread can access a shared resource at a time, preventing race conditions.\n\ne.g:\nimport threading\n\nlock = threading.Lock()\n\ndef thread_safe_function():\n    with lock:\n        # Critical section\n2. threading.RLock (Reentrant Lock):\n\nSimilar to Lock, but can be acquired multiple times by the same thread.\n\ne.g:\nimport threading\n\nlock = threading.RLock()\n\ndef thread_safe_function():\n    with lock:\n        # Critical section\n3. threading.Event:\n\nUsed for signaling between threads.\n\ne.g:\nimport threading\n\nevent = threading.Event()\n\ndef wait_for_event():\n    event.wait()  # Block until the event is set\n\ndef set_event():\n    event.set()  # Set the event\nFor Processes:\n1. multiprocessing.Queue:\n\nProvides a FIFO (first-in, first-out) queue that can be shared between processes.\n\ne.g:\nimport multiprocessing\n\nqueue = multiprocessing.Queue()\n\ndef producer(q):\n    q.put('data')\n\ndef consumer(q):\n    data = q.get()\n2. multiprocessing.Pipe:\n\nAllows bidirectional communication between two processes.\n\ne.g:\nimport multiprocessing\n\nparent_conn, child_conn = multiprocessing.Pipe()\n\ndef send_data(conn):\n    conn.send('data')\n\ndef receive_data(conn):\n    data = conn.recv()\n3. multiprocessing.Value and multiprocessing.Array:\n\nProvide shared memory for simple data types and arrays, respectively.\n\ne.g:\nimport multiprocessing\n\nshared_value = multiprocessing.Value('i', 0)  # 'i' indicates an integer\n\ndef increment_value(val):\n    with val.get_lock():\n        val.value += 1\n4. multiprocessing.Manager:\n\nManages a server process to create shared data structures like dictionaries and lists.\n\ne.g:\nimport multiprocessing\n\nmanager = multiprocessing.Manager()\nshared_dict = manager.dict()\n\ndef update_dict(d):\n    d['key'] = 'value",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Q 6  Discuss why it’s crucial to handle exceptions in concurrent programs and the techniques available for\ndoing so.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "Handling exceptions in concurrent programs is crucial because it helps maintain the stability and reliability of your software. When you have multiple threads or processes running simultaneously, the complexity increases, and so does the likelihood of errors. If an exception occurs in one part of your concurrent program and isn't properly managed, it can crash your entire application or lead to unpredictable behavior.\n\nWhy it's crucial:\n\n1. Avoid Crashes: Unhandled exceptions can crash your entire application, halting all concurrent tasks.\n\n2. Resource Leaks: They can cause resources like file handles, memory, or network connections to be left open.\n\n3. Data Integrity: Shared data might end up in an inconsistent state if exceptions aren't handled properly.\n\n4. Debugging: Proper exception handling helps diagnose issues by logging or reporting errors in a controlled manner.\nTechniques available in Python:\n\nTry-Except Blocks:\n\nWrap critical sections of code in try-except blocks to catch and handle exceptions.\ne.g:\ntry:\n    # Critical section\nexcept SomeException as e:\n    # Handle exception\n2. Threading:\n\nUse threading.Thread with exception handling.\n\ne.g:\nimport threading\n\ndef thread_function():\n    try:\n        # Thread-specific code\n    except Exception as e:\n        print(f\"Error in thread: {e}\")\n\nthread = threading.Thread(target=thread_function)\nthread.start()\nthread.join()\n3. Multiprocessing:\n\nHandle exceptions in processes and use multiprocessing.Queue to communicate errors.\n\ne.g:\nimport multiprocessing\n\ndef worker(queue):\n    try:\n        # Process-specific code\n    except Exception as e:\n        queue.put(e)\n\nerror_queue = multiprocessing.Queue()\nprocess = multiprocessing.Process(target=worker, args=(error_queue,))\nprocess.start()\nprocess.join()\n\nif not error_queue.empty():\n    error = error_queue.get()\n    print(f\"Error in process: {error}\")\n4. Timeouts:\n\nUse timeouts to prevent tasks from running indefinitely.\n\ne.g:\nimport threading\n\ndef thread_function():\n    try:\n        # Thread-specific code\n    except Exception as e:\n        print(f\"Error in thread: {e}\")\n\nthread = threading.Thread(target=thread_function)\nthread.start()\nthread.join(timeout=5)  # Wait for 5 seconds\n\nif thread.is_alive():\n    print(\"Thread timed out\")\n5. Context Managers:\n\nUse context managers to ensure that resources are properly released even if an exception occurs.\n\ne.g:\nfrom contextlib import contextmanager\n\n@contextmanager\ndef resource_manager():\n    # Setup code\n    try:\n        yield\n    finally:\n        # Cleanup code\n\nwith resource_manager():\n    # Critical section\n\nThese techniques help manage exceptions gracefully, ensure resources are cleaned up properly, and keep your concurrent programs running smoothly.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Q 7  Create a program that uses a thread pool to calculate the factorial of numbers from 1 to 10 concurrently.\nUse concurrent.futures.ThreadPoolExecutor to manage the threads.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "from concurrent.futures import ThreadPoolExecutor\nimport math\n\n# Function to calculate factorial\ndef calculate_factorial(n):\n    return math.factorial(n)\n\nif _name_ == \"_main_\":\n    numbers = range(1, 11)\n    \n    # Create a ThreadPoolExecutor\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        # Submit tasks to the thread pool\n        futures = {executor.submit(calculate_factorial, num): num for num in numbers}\n        \n        for future in futures:\n            num = futures[future]\n            try:\n                result = future.result()\n                print(f\"Factorial of {num} is {result}\")\n            except Exception as e:\n                print(f\"An error occurred while calculating the factorial of {num}: {e}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# 8 Create a Python program that uses multiprocessing.Pool to compute the square of numbers from 1 to 10 in\nparallel. Measure the time taken to perform this computation using a pool of different sizes (e.g., 2, 4, 8\nprocesses).",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "\nfrom multiprocessing import Pool\nimport time\n\n# Function to compute square\ndef compute_square(n):\n    return n * n\n\ndef measure_time(pool_size):\n    numbers = list(range(1, 11))\n    start_time = time.time()\n    \n    with Pool(pool_size) as pool:\n        results = pool.map(compute_square, numbers)\n    \n    end_time = time.time()\n    time_taken = end_time - start_time\n    return time_taken, results\n\nif _name_ == \"_main_\":\n    pool_sizes = [2, 4, 8]\n    \n    for size in pool_sizes:\n        time_taken, results = measure_time(size)\n        print(f\"Pool size: {size} | Time taken: {time_taken:.4f} seconds | Results: {results}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}